{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing a document\n",
    "\n",
    "First, we will load the document and extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"long_pdf.pdf\") as pdf:\n",
    "    pdf_text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        pdf_text += str(page.extract_text())\n",
    "\n",
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to count the amount of tokens.\n",
    "\n",
    "To accomplish this we will use the Openai's `tiktoken`library, using the GPT-4 tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amount of GPT-4 tokens: 258487'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "tokens = encoding.encode(pdf_text)\n",
    "\n",
    "f'Amount of GPT-4 tokens: {len(tokens)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to implement a cheap and efficient solution.\n",
    "\n",
    "Since the tokens length is very big (250k+), we can analyze three different approaches:\n",
    "\n",
    "- `Prompt Stuffing`\n",
    "- `Map Reduce`\n",
    "- `Refine`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Stuffing\n",
    "\n",
    "> The stuff documents chain (\"stuff\" as in \"to stuff\" or \"to fill\") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.\n",
    "\n",
    "In the context of document summarization, the prompt stuffing would be simply pass all the documents text as context in a single prompt, for example:\n",
    "```py\n",
    "\"\"\"\n",
    "Summarize the document:\n",
    "\n",
    "{document_text}\n",
    "\"\"\" \n",
    "```\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Efficient (Single LLM call)\n",
    "- No context loss\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- `Context window`: The context window is the amount of tokens a LLM is capable to receive as input. The most part of the models cannot receive 250k+ tokens.\n",
    "- Probably expensive, due to the need of using a more robust model.\n",
    "\n",
    "> Google already has models (in preview) that supports a large amount of tokens as input. However, for prompts greater than 128k tokens, double the price is charged\n",
    "\n",
    "## Map Reduce\n",
    "\n",
    "The Map Reduce technique is a method for processing large datasets in parallel across a cluster of computers. In the context of document summarization, Map Reduce can be used to break down a large document into smaller chunks, process each chunk independently, and then combine the results to produce a summary.\n",
    "\n",
    "The \"Map\" step involves dividing the document into smaller chunks and processing each chunk in parallel. This can be done by creating a prompt for each chunk and passing it to a language model to generate a summary.\n",
    "\n",
    "The \"Reduce\" step involves combining the summaries from each chunk to produce a final summary. This can be done by concatenating the summaries, ranking them, or using other methods to combine the results.\n",
    "\n",
    "By breaking down the document into smaller chunks, it is possible to process the document in parallel and produce a summary that is more comprehensive and accurate.\n",
    "\n",
    "Pros:\n",
    "- Parallelism (although the need of multiple LLM calls)\n",
    "\n",
    "Limitations:\n",
    "- Context loss: each chunk is processed separately, so the chunks summaries are not truly connected\n",
    "\n",
    "\n",
    "## Refine \n",
    "The Refine technique involves refining the initial summary generated by the LLM. This can be done by applying various post-processing techniques such as re-ranking the sentences, removing redundant information, or adding additional context.\n",
    "\n",
    "Pros:\n",
    "- Improved summary quality\n",
    "\n",
    "Limitations:\n",
    "- Requires additional processing steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
